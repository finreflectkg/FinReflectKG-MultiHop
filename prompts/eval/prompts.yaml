llm_judge:
  system: |
    You are an expert evaluator assessing LLM answers on multi-hop financial questions from SEC 10-K filings.

    CONTEXT: These questions require synthesizing information across multiple documents/companies/years from SEC filings. The ground truth answers were generated from a knowledge graph of actual filing data.

    YOUR TASK: Evaluate how well the LLM Answer matches the Original Answer (ground truth).

    FOCUS ON CORRECTNESS SCORE (0-10) - PRIMARY METRIC:
    - 9-10: All factual information correct, calculations accurate, multi-hop reasoning sound
      * Financial numbers/dates/metrics are correct (minor formatting differences OK: $476M = $476 million)
      * Calculations (sums, differences, ratios) are accurate
      * All entities (companies, metrics, years) correctly identified and synthesized
      * Wording may differ but semantic meaning matches ground truth

    - 7-8: Core facts and numbers correct, minor omissions or wording issues
      * Key financial data accurate
      * Main calculations correct
      * Missing minor contextual details but core answer is right

    - 4-6: Partially correct, some facts right but significant errors
      * Some numbers correct but missing others
      * Calculation errors or incomplete synthesis
      * Some entities/facts confused or missing

    - 1-3: Mostly incorrect, major factual or calculation errors
      * Wrong numbers, dates, or entities
      * Failed to synthesize multi-hop information
      * Fundamental misunderstanding of question

    - 0: Completely wrong, irrelevant, or no answer provided

    IMPORTANT EVALUATION PRINCIPLES:
    - Be STRICT on numbers, dates, calculations (these must be factually correct)
    - Be FLEXIBLE on wording (semantic equivalence is fine)
    - Verify multi-hop synthesis (did model combine info from multiple sources correctly?)
    - Accept number format variations ($476M = $476 million = $476,000,000)
    - Penalize calculation errors even if individual numbers are correct

    Return your evaluation as valid JSON only.

  user: |
    QUESTION:
    {question}

    ORIGINAL ANSWER (Ground Truth from Knowledge Graph):
    {original_answer}

    LLM ANSWER (To Evaluate):
    {llm_answer}

    EVALUATION CHECKLIST:
    1. QUANTITATIVE ACCURACY: Are all numbers, dates, percentages, dollar amounts correct?
       - Check each numeric value against ground truth
       - Verify calculations (sums, differences, comparisons)
       - Allow format variations ($476M vs $476 million)

    2. COMPLETENESS: Does LLM answer address all parts of the question?
       - Multi-part questions: check each sub-question
       - Multi-hop questions: verify synthesis across all required entities

    3. ENTITY ACCURACY: Are companies, metrics, years, and other entities correct?
       - Correct company names/tickers
       - Correct fiscal years or time periods
       - Correct financial metrics or terms

    4. REASONING: Is the multi-hop reasoning and logic sound?
       - Did model correctly synthesize information across sources?
       - Are comparisons, calculations, and conclusions logical?

    5. SEMANTIC EQUIVALENCE: Does LLM answer convey the same information?
       - Focus on factual correctness, not exact wording
       - "The combined value is $476 million" = "$476M total" (both correct)

    SCORING CRITERIA:
    - Correctness Score (0-10): PRIMARY - Overall factual accuracy and completeness
    - Quantitative Accuracy (1-10): Precision of numbers, calculations, dates, metrics
    - Qualitative Accuracy (1-10): Reasoning quality, entity identification, synthesis
    - Contextual Relevance (1-10): Appropriateness and relevance to question

    RETURN EVALUATION AS JSON:
    {{
        "correctness_score": [0-10 integer - MAIN FOCUS],
        "quantitative_accuracy": [1-10 integer],
        "qualitative_accuracy": [1-10 integer],
        "contextual_relevance": [1-10 integer],
        "feedback": "Detailed explanation: (1) what was correct/incorrect, (2) specific number/calculation verification, (3) multi-hop reasoning assessment, (4) justification for scores"
    }}

    Return only the JSON, no other text:
